---
title: SfMLearner学习
layout: post
tags: []
category: Uncategoried
---
## Approach

从无标签的视频序列中训练一个深度估计CNN和相机姿态估计CNN。虽然是一起训练的，但是得到的深度的模型和姿态的模型能够在测试时独立运行。

#### 综合视图监督
深度和姿态预测CNNs的核心的监督信息来源自novel view synthesis：给定场景的一个视图，合成一个相机在另一个不同姿态下的新图像。我们通过合成目标视图的深度图、这个时刻的pose和附近视角下的图像。而这个合成的步骤能通过CNNs的全可微操作实现。
令$$<I_1,I_2,...,I_N>$$作为待训练图像序列的输入，其中$$I_t$$是目标视图，其他的图像序列为源视图$$I_s(1\le s\le N,s\ne t)$$。那么视图生成目标函数能通过下式表示：

$$\mathcal{L_{vs}}=\sum_s\sum_p|I_t(p_)-\hat{I}_s(p)|$$

其中$$p$$表示像素坐标，$$\hat{I}_{s}$$表示源视图$$I_s$$根据深度图像渲染模型（具体见下一小结）warped到目标坐标下的视图。这个模型以预测到的深度$$\hat{D}_t$$,预测的4*4的相机变换矩阵$$\hat{T}_t \to s$$和源视图的$$I_s$$作为输入。

这篇文章与其他方法相比是预先不需要pose的信息。同时能够将预测的pose作为学习框架一部分。

#### 可微深度图像渲染
上一节介绍到的学习框架中一个重要的组成是基于深度图$$\hat{D}_t$$和相对姿态$$\hat{T}_s$$对源视图$$I_s$$的像素采样后创建目标视图$$I_t$$，这个方式是可微的深度图像渲染。
令$$p_t$$代表目标视图的一个像素的单应性矩阵，$$K$$代表相机的内参，那么可以通过下式子把观测的$$p_t$$映射到源视图$$p_s$$中：

$$p_s\backsim K\hat{T}_t\to s\hat{D}_t(p_t)K^{-1}p_t$$

注意到投影的坐标$${p_s}$$是连续值。可以利用可微双边采样，线性插值$$p_s$$四个相邻像素的值来近似$$I_s(p_s)$$：$$\hat{I}_s(p_t)=I_s(p_s)=\sum_{i\in{t,b}},j\in{l,r} \omega{} $$
#### 建模模型限制
#### 克服梯度的局部性
#### 网络结构