---
title: SfMLearnerNote
layout: post
tags: []
category: Uncategoried
---

# SfMLearner学习笔记
## Approach

从无标签的视频序列中训练一个深度估计CNN和相机姿态估计CNN。虽然是一起训练的，但是得到的深度的模型和姿态的模型能够在测试时独立运行。

#### 综合视图监督
深度和姿态预测CNNs的核心的监督信息来源自*novel view synthesis*：给定场景的一个视图，合成一个相机在另一个不同姿态下的新图像。我们通过合成目标视图的深度图、这个时刻的pose和附近视角下的图像。而这个合成的步骤能通过CNNs的全可微操作实现。
令$<I_1,I_2,...,I_N>$作为待训练图像序列的输入，其中$I_t$是目标视图，其他的图像序列为源视图$I_s(1\le s\le N,s\ne t)$。那么视图生成目标函数能通过下式表示：

$$\mathcal{L_{vs}}=\sum_s\sum_p|I_t(p_)-\hat{I}_s(p)|$$

其中$p$表示像素坐标，$\hat{I}_{s}$表示源视图$I_s$根据深度图像渲染模型（具体见下一小结）warped到目标坐标下的视图。这个模型以预测到的深度$\hat{D}_t$,预测的4*4的相机变换矩阵$\hat{T}_t \to s$和源视图的$I_s$作为输入。

这篇文章与其他方法相比是预先不需要pose的信息。同时能够将预测的pose作为学习框架一部分。

#### 可微深度图像渲染
上一节介绍到的学习框架中一个重要的组成是基于深度图$\hat{D}_t$和相对姿态$\hat{T}_s$对源视图$I_s$的像素采样后创建目标视图$I_t$，这个方式是可微的深度图像渲染。
令$p_t$代表目标视图的一个像素的单应性矩阵，$K$代表相机的内参，那么可以通过下式子把观测的$p_t$映射到源视图$p_s$中：

$$p_s\backsim K\hat{T}_t\to s\hat{D}_t(p_t)K^{-1}p_t$$

注意到投影的坐标${p_s}$是连续值。可以利用可微双边采样，线性插值$p_s$四个相邻像素的值来近似$I_s(p_s)$,用一个$\omega ^{ij}$表示线性插值。此处通过投影几何学得到的像素warping的坐标能够充分利用估计的深度和估计的相机姿态。
#### 建模模型限制
考虑到当我们将上述视图合成公式应用到单目视频中时我们做了如下假设：
1、场景是静态的（没有运动物体）
2、源视图与目标视图之间没有遮挡
3、表面是理想镜面反射，图像的一致性误差才有意义。
上述三个假设中在训练序列不成立是，那么求梯度就会奔溃，可能导致训练失败。为了我们的pipeline对这些问题具有鲁棒性，我们额外训练了一个*explainability prediction*网络（与深度位姿估计网络一起同时进行）。这个网络能够得到每个目标视图对的每个像素的soft mask $\hat{E}_s$，这意味着该网络对目标视图的每个像素的置信系数。于是我们将合成视图的目标函数与该权重相乘：

$$\mathcal{L}_{vs}=\sum\sum_p \hat{E}_s(p)|I_t(p_)-\hat{I}_s(p)|$$

这里又会出现一个问题，上式中我们没有对$\hat{E}_s$有直接的监督，这样会导致网络永远把$\hat{E}_s$预测为0，此时*loss*最小。为了解决这个问题，我们对$\hat{E}_s$加一个正则项，使其成为$\mathcal{L}_r(\hat{E}_s)$，这个正则项通过最小化交叉熵损失来鼓励非零的预测。换言之，网络鼓励最小化上述的方程，但是同时对模型没考虑的因素允许一定程度的放宽政策。

#### 克服梯度的局部性
还有一个没有考虑的问题是梯度主要来源于$I(p_t)$与其四个相邻的$I(p_s)$,这有可能导致训练失败如果$p_s$位于一个少纹理的区域或者与当前估计相去甚远。这在运动估计中时一个经典的问题。凭借我们的经验，我们考虑了两个策略来解决这个问题：
1、利用网络结构约束输出平滑，使得梯度从有意义的区域传输到临近的区域；
2、明确多尺度与平滑损失：允许梯度能直接从更大的空间区域得到。通过将第二种思路应用到该工作中，因为它对网络结构框架不敏感，预测深度图时，我们通过最小化二阶梯度的$L_1$范数来得到。这个思路与文章48相似。
我们最后的目标函数：

$$\mathcal{L}_{final}=\sum_l\mathcal{L}_{vs}^l+\lambda _s\mathcal{L}_{smooth}^l+\lambda_e \sum_s \mathcal{L}_r(\hat{E}_s^l)$$

#### 网络结构

##### single-view depth
单图像的预测，利用文章36中的*DispNet*网络结构。除了预测层，所有的卷积层都根据ReLU激活方式。预测层利用公式：$1/(\alpha*sigmoid(x)+\beta)$其中$\alpha=10$、$\beta=0.1$，用来约束预测的深度一直为正数并且都在一个可靠的范围内。（当然作者也利用过多视图做为输入，但是并没有发现这样的输入能够改进预测的结果，这与文章47的结论一致）。
##### Pose
Pose estimation network 的输入是目标视图及与其有关的所有源视图。输出是目标视图与每个源视图之间的相对关系。
网络包含7stride-2convolutions，$6\times (N-1)$输出channel（其中包含到每个源视图的3个欧拉角和3维的平移矩阵），最后，对所有的空间上的预测进行全局平局的pooling。这里面除了最后一层，所有的卷积层都由ReLU激活。
##### Explainablity mask
*explainable prediction network*跟*pose network*共用最开始的五个特征编码层，而后连接5个包含多尺度预测的逆卷积层。除了最后的预测层外，所有的卷积/逆卷积层同样都由ReLU激活。每个预测层的输出channel为$2\times (N-1)$，每两个channel有*softmax*进行正则化得到源-目标关联对的“可解释”预测（第二个channel正则化后就是$\hat{E}_s$，能用来计算前面最后目标函数的*loss*）。
